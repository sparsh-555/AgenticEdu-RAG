"""
Unified RAG Retrieval System for Educational Content

This module implements sophisticated retrieval strategies for educational content
using a unified vector store with metadata-based filtering. It replaces the previous
multi-collection approach with intelligent content filtering based on rich metadata.

Key Features:
1. Unified Search Strategy: Single collection search with metadata filtering
2. Educational Relevance Scoring: Content quality and appropriateness for learning context
3. Context-Aware Filtering: Intelligent filtering based on agent needs and SRL phases
4. Adaptive Retrieval: Learning from retrieval effectiveness to improve future results
5. Performance Optimization: Caching, batch processing, and efficient search strategies
6. Rich Metadata Utilization: Leveraging comprehensive metadata for precise content matching

Unified Retrieval Pipeline:
1. Query Analysis: Understand information need and educational context
2. Metadata Filter Construction: Build intelligent filters for unified collection
3. Unified Search: Single collection search with comprehensive filtering
4. Educational Scoring: Score candidates based on educational appropriateness
5. Context-Aware Ranking: Final ranking considering all factors
6. Response Generation: Format results for agent consumption
"""

import time
import math
from typing import Dict, List, Optional, Any, Tuple, Set, Union
from dataclasses import dataclass, field
from enum import Enum
import json
import re
from collections import defaultdict, Counter

from pydantic import BaseModel, Field

from .vector_store import (
    UnifiedEducationalVectorStore, get_vector_store, RetrievalResult,
    ContentType, AgentSpecialization
)
from classification.srl_classifier import SRLPhase
from utils.logging_utils import get_logger, LogContext, EventType, create_context
from config.settings import get_settings


class UnifiedRetrievalStrategy(Enum):
    """Different retrieval strategies for unified collection."""
    METADATA_FILTERED = "metadata_filtered"
    CONCEPT_FOCUSED = "concept_focused"
    EDUCATIONAL_RELEVANCE = "educational_relevance"
    CONTEXTUAL_ADAPTIVE = "contextual_adaptive"
    AGENT_SPECIALIZED = "agent_specialized"


class UnifiedRetrievalContext(BaseModel):
    """Context information for unified retrieval operations."""
    query: str = Field(..., description="User query for retrieval")
    agent_specialization: Optional[str] = Field(default=None, description="Target agent specialization")
    srl_phase: Optional[str] = Field(default=None, description="Self-regulated learning phase")
    student_level: Optional[str] = Field(default=None, description="Student proficiency level")
    
    # Query context
    code_snippet: Optional[str] = Field(default=None, description="Associated code snippet")
    error_message: Optional[str] = Field(default=None, description="Associated error message")
    conversation_history: Optional[List[Dict[str, str]]] = Field(default=None, description="Previous conversation")
    
    # Retrieval preferences
    max_results: int = Field(default=5, description="Maximum results to return")
    similarity_threshold: float = Field(default=0.0, description="Minimum similarity threshold")
    prefer_code_examples: bool = Field(default=False, description="Prefer content with code examples")
    prefer_error_examples: bool = Field(default=False, description="Prefer content with error examples")
    
    # Educational context
    programming_domain: Optional[str] = Field(default=None, description="Programming domain focus")
    learning_objectives: Optional[List[str]] = Field(default=None, description="Specific learning objectives")
    content_type_preference: Optional[str] = Field(default=None, description="Preferred content type")


@dataclass
class UnifiedScoredResult:
    """A retrieval result with comprehensive scoring for unified approach."""
    result: RetrievalResult
    
    # Scoring components
    similarity_score: float = 0.0
    metadata_match_score: float = 0.0
    educational_relevance_score: float = 0.0
    context_match_score: float = 0.0
    agent_specialization_score: float = 0.0
    quality_score: float = 0.0
    freshness_score: float = 0.0
    
    # Final scores
    combined_score: float = 0.0
    rank_position: int = 0
    
    # Scoring explanation
    score_explanation: Dict[str, float] = field(default_factory=dict)


class UnifiedRetrievalMetrics(BaseModel):
    """Metrics for unified retrieval performance monitoring."""
    total_retrievals: int = Field(default=0, description="Total retrieval operations")
    average_retrieval_time_ms: float = Field(default=0.0, description="Average retrieval time")
    average_results_returned: float = Field(default=0.0, description="Average number of results")
    
    # Quality metrics
    average_similarity_score: float = Field(default=0.0, description="Average similarity score")
    average_metadata_match_score: float = Field(default=0.0, description="Average metadata match score")
    average_educational_relevance: float = Field(default=0.0, description="Average educational relevance")
    
    # Strategy effectiveness
    strategy_usage: Dict[str, int] = Field(default_factory=dict, description="Usage count per strategy")
    strategy_performance: Dict[str, float] = Field(default_factory=dict, description="Performance per strategy")
    
    # Metadata filter effectiveness
    filter_usage: Dict[str, int] = Field(default_factory=dict, description="Usage count per filter type")
    filter_effectiveness: Dict[str, float] = Field(default_factory=dict, description="Effectiveness per filter")


class UnifiedEducationalRetriever:
    """
    Unified retrieval system for educational content.
    
    This retriever uses a single vector collection with intelligent metadata
    filtering to provide highly relevant content for programming education.
    It replaces the previous multi-collection approach with a more flexible
    and maintainable unified architecture.
    """
    
    def __init__(self):
        """Initialize the unified educational retriever."""
        self.settings = get_settings()
        self.logger = get_logger()
        self.vector_store = get_vector_store()
        
        # Unified retrieval strategy weights
        self.strategy_weights = self._initialize_strategy_weights()
        
        # Educational scoring weights for unified approach
        self.educational_weights = self._initialize_educational_weights()
        
        # Metadata filtering weights
        self.metadata_weights = self._initialize_metadata_weights()
        
        # Performance tracking
        self.retrieval_metrics = UnifiedRetrievalMetrics()
        self.strategy_performance_history = defaultdict(list)
        
        # Concept similarity mappings
        self.concept_similarity = self._initialize_concept_similarity()
        
        # Keyword expansion dictionaries
        self.keyword_expansions = self._initialize_keyword_expansions()
        
        self.logger.log_event(
            EventType.COMPONENT_INIT,
            "Unified educational retriever initialized",
            extra_data={"retrieval_strategies": len(self.strategy_weights)}
        )
    
    def retrieve_educational_content(self, 
                                   context: UnifiedRetrievalContext,
                                   log_context: Optional[LogContext] = None) -> List[UnifiedScoredResult]:
        """
        Retrieve and score educational content using unified collection approach.
        
        Args:
            context: Unified retrieval context with query and educational metadata
            log_context: Logging context for tracking
            
        Returns:
            List of scored and ranked retrieval results
        """
        log_context = log_context or create_context()
        start_time = time.time()
        
        try:
            # DEBUG: Log the incoming context
            self.logger.log_event(
                EventType.KNOWLEDGE_RETRIEVED,
                "retrieve_educational_content called",
                extra_data={
                    "agent_specialization": context.agent_specialization,
                    "agent_specialization_type": type(context.agent_specialization).__name__,
                    "context_attributes": {
                        attr: getattr(context, attr) for attr in dir(context) 
                        if not attr.startswith('_') and hasattr(context, attr)
                    }
                }
            )
            
            # Analyze query to determine optimal unified strategy
            strategy = self._select_unified_retrieval_strategy(context)
            
            # Enhance query with educational context
            enhanced_query = self._enhance_query_with_context(context)
            
            # Build comprehensive metadata filters for unified collection
            self.logger.log_event(
                EventType.KNOWLEDGE_RETRIEVED,
                "Building metadata filters",
                extra_data={"context_agent_specialization": context.agent_specialization}
            )
            metadata_filters = self._build_comprehensive_metadata_filters(context)
            self.logger.log_event(
                EventType.KNOWLEDGE_RETRIEVED,
                "Metadata filters built successfully",
                extra_data={"filters": metadata_filters}
            )
            
            # Retrieve candidates using unified approach
            try:
                candidates = self._retrieve_from_unified_collection(
                    enhanced_query, context, metadata_filters, strategy
                )
                self.logger.log_event(
                    EventType.KNOWLEDGE_RETRIEVED,
                    f"Retrieved {len(candidates)} candidates successfully"
                )
            except Exception as e:
                self.logger.log_event(
                    EventType.ERROR_OCCURRED,
                    f"Error in _retrieve_from_unified_collection: {str(e)}",
                    level="ERROR"
                )
                raise
            
            # Score candidates using unified educational metrics
            try:
                scored_results = self._score_unified_educational_relevance(candidates, context)
                self.logger.log_event(
                    EventType.KNOWLEDGE_RETRIEVED,
                    f"Scored {len(scored_results)} results successfully"
                )
            except Exception as e:
                self.logger.log_event(
                    EventType.ERROR_OCCURRED,
                    f"Error in _score_unified_educational_relevance: {str(e)}",
                    level="ERROR"
                )
                raise
            
            # Apply advanced contextual filtering
            try:
                filtered_results = self._apply_unified_contextual_filtering(scored_results, context)
                self.logger.log_event(
                    EventType.KNOWLEDGE_RETRIEVED,
                    f"Filtered to {len(filtered_results)} results"
                )
            except Exception as e:
                self.logger.log_event(
                    EventType.ERROR_OCCURRED,
                    f"Error in _apply_unified_contextual_filtering: {str(e)}",
                    level="ERROR"
                )
                raise
            
            # Final ranking and selection with unified scoring
            try:
                final_results = self._unified_final_ranking_and_selection(filtered_results, context)
                self.logger.log_event(
                    EventType.KNOWLEDGE_RETRIEVED,
                    f"Final ranking produced {len(final_results)} results"
                )
            except Exception as e:
                self.logger.log_event(
                    EventType.ERROR_OCCURRED,
                    f"Error in _unified_final_ranking_and_selection: {str(e)}",
                    level="ERROR"
                )
                raise
            
            # Update unified performance metrics
            retrieval_time = (time.time() - start_time) * 1000
            self._update_unified_retrieval_metrics(strategy, final_results, retrieval_time, metadata_filters)
            
            # Log unified retrieval operation
            self.logger.log_rag_operation(
                operation="unified_educational_retrieval",
                query=context.query,
                results_count=len(final_results),
                context=log_context
            )
            
            self.logger.log_event(
                EventType.KNOWLEDGE_RETRIEVED,
                f"Unified educational content retrieved: {len(final_results)} results",
                context=log_context,
                extra_data={
                    "strategy": strategy.value,
                    "retrieval_time_ms": retrieval_time,
                    "enhanced_query_length": len(enhanced_query),
                    "metadata_filters_applied": len(metadata_filters),
                    "candidates_retrieved": len(candidates),
                    "final_results": len(final_results)
                }
            )
            
            return final_results
            
        except Exception as e:
            import traceback
            self.logger.log_event(
                EventType.ERROR_OCCURRED,
                f"Unified educational retrieval failed: {str(e)}",
                context=log_context,
                level="ERROR",
                extra_data={"query": context.query[:100], "traceback": traceback.format_exc()}
            )
            return []
    
    def _select_unified_retrieval_strategy(self, context: UnifiedRetrievalContext) -> UnifiedRetrievalStrategy:
        """
        Select optimal unified retrieval strategy based on context.
        
        Args:
            context: Unified retrieval context
            
        Returns:
            Selected unified retrieval strategy
        """
        strategy_scores = {}
        
        # Agent-specialized strategy for specific agent requests
        if context.agent_specialization:
            strategy_scores[UnifiedRetrievalStrategy.AGENT_SPECIALIZED] = 0.9
        
        # Concept-focused strategy for conceptual queries
        if any(word in context.query.lower() for word in ["concept", "understand", "explain", "theory"]):
            strategy_scores[UnifiedRetrievalStrategy.CONCEPT_FOCUSED] = 0.85
        
        # Educational relevance for learning-focused queries
        if any(word in context.query.lower() for word in ["learn", "practice", "exercise", "example"]):
            strategy_scores[UnifiedRetrievalStrategy.EDUCATIONAL_RELEVANCE] = 0.8
        
        # Contextual adaptive for complex queries with multiple factors
        complexity_indicators = [
            bool(context.code_snippet),
            bool(context.error_message),
            bool(context.conversation_history),
            len(context.query.split()) > 10,
            bool(context.programming_domain),
            bool(context.learning_objectives)
        ]
        
        if sum(complexity_indicators) >= 3:
            strategy_scores[UnifiedRetrievalStrategy.CONTEXTUAL_ADAPTIVE] = 0.95
        
        # Select strategy with highest score, default to metadata filtered
        if strategy_scores:
            return max(strategy_scores, key=strategy_scores.get)
        else:
            return UnifiedRetrievalStrategy.METADATA_FILTERED
    
    def _enhance_query_with_context(self, context: UnifiedRetrievalContext) -> str:
        """
        Enhance query with educational context for better unified retrieval.
        
        Args:
            context: Unified retrieval context
            
        Returns:
            Enhanced query string
        """
        query_parts = [context.query]
        
        # Add SRL phase context
        if context.srl_phase:
            if context.srl_phase == SRLPhase.FORETHOUGHT.value:
                query_parts.append("planning implementation strategy approach design")
            elif context.srl_phase == SRLPhase.PERFORMANCE.value:
                query_parts.append("debugging error fixing troubleshooting performance")
        
        # Add agent specialization context
        if context.agent_specialization:
            if context.agent_specialization == AgentSpecialization.IMPLEMENTATION.value:
                query_parts.append("implementation design approach methodology")
            elif context.agent_specialization == AgentSpecialization.DEBUGGING.value:
                query_parts.append("debugging troubleshooting error fixing")
        
        # Add student level context
        if context.student_level:
            query_parts.append(f"{context.student_level} level difficulty")
        
        # Add programming domain context
        if context.programming_domain:
            query_parts.append(context.programming_domain)
        
        # Add content type preference
        if context.content_type_preference:
            query_parts.append(context.content_type_preference)
        
        # Add code context
        if context.code_snippet:
            # Extract key terms from code
            code_terms = self._extract_code_terms(context.code_snippet)
            query_parts.extend(code_terms[:3])  # Add top 3 terms
        
        # Add error context
        if context.error_message:
            # Extract error type
            error_type = self._extract_error_type(context.error_message)
            if error_type:
                query_parts.append(error_type)
        
        # Add learning objectives
        if context.learning_objectives:
            query_parts.extend(context.learning_objectives[:2])  # Add top 2 objectives
        
        # Expand keywords for unified search
        expanded_query = self._expand_keywords(" ".join(query_parts))
        
        return expanded_query
    
    def _build_comprehensive_metadata_filters(self, context: UnifiedRetrievalContext) -> Dict[str, Any]:
        """
        Build comprehensive metadata filters for unified collection search.
        
        Args:
            context: Unified retrieval context
            
        Returns:
            Dictionary of metadata filters
        """
        filters = {}
        
        # DEBUGGING: Log input context details
        self.logger.log_event(
            EventType.KNOWLEDGE_RETRIEVED,
            "Building metadata filters - INPUT CONTEXT",
            extra_data={
                "agent_specialization": context.agent_specialization,
                "student_level": context.student_level,
                "content_type_preference": context.content_type_preference,
                "srl_phase": context.srl_phase,
                "prefer_code_examples": context.prefer_code_examples,
                "prefer_error_examples": context.prefer_error_examples,
                "query": context.query[:100]
            }
        )
        
        # Agent specialization filter - RESEARCH FIX: Don't filter at this level for better research demo
        # The agent specialization will be handled through scoring/ranking rather than hard filtering
        if context.agent_specialization:
            self.logger.log_event(
                EventType.KNOWLEDGE_RETRIEVED,
                f"RESEARCH: Agent specialization noted for scoring (not filtering): {context.agent_specialization}"
            )
            # Don't add to filters - let scoring system handle preference
        
        # Difficulty level filter - RESEARCH FIX: Make this a preference rather than hard filter
        if context.student_level:
            self.logger.log_event(
                EventType.KNOWLEDGE_RETRIEVED,
                f"RESEARCH: Difficulty level noted for scoring (not hard filtering): {context.student_level}"
            )
            # Don't add strict difficulty filter - let scoring system prefer appropriate levels
        
        # Content type filter - RESEARCH FIX: Use as preference for scoring, not hard filter
        if context.content_type_preference:
            self.logger.log_event(
                EventType.KNOWLEDGE_RETRIEVED,
                f"RESEARCH: Content type preference noted for scoring: {context.content_type_preference}"
            )
            # Don't add hard filter - let scoring system prefer the type
        elif context.srl_phase:
            if context.srl_phase == SRLPhase.FORETHOUGHT.value:
                self.logger.log_event(
                    EventType.KNOWLEDGE_RETRIEVED,
                    f"RESEARCH: SRL phase (forethought) noted - will prefer implementation content in scoring"
                )
            elif context.srl_phase == SRLPhase.PERFORMANCE.value:
                self.logger.log_event(
                    EventType.KNOWLEDGE_RETRIEVED,
                    f"RESEARCH: SRL phase (performance) noted - will prefer debugging content in scoring"
                )
        
        # Code examples preference - RESEARCH FIX: Use as scoring preference
        if context.prefer_code_examples:
            self.logger.log_event(
                EventType.KNOWLEDGE_RETRIEVED,
                "RESEARCH: Code examples preference noted for scoring (not hard filtering)"
            )
            # Don't require code examples - let scoring system prefer them
        
        # Error examples preference - RESEARCH FIX: Use as scoring preference  
        if context.prefer_error_examples:
            self.logger.log_event(
                EventType.KNOWLEDGE_RETRIEVED,
                "RESEARCH: Error examples preference noted for scoring (not hard filtering)"
            )
            # Don't require error examples - let scoring system prefer them
        
        # DEBUGGING: Log final filter set
        self.logger.log_event(
            EventType.KNOWLEDGE_RETRIEVED,
            "FINAL METADATA FILTERS BUILT",
            extra_data={
                "filters_applied": filters,
                "filter_count": len(filters),
                "query_context": context.query[:100]
            }
        )
        
        return filters
    
    def _retrieve_from_unified_collection(self, 
                                        enhanced_query: str,
                                        context: UnifiedRetrievalContext,
                                        metadata_filters: Dict[str, Any],
                                        strategy: UnifiedRetrievalStrategy) -> List[RetrievalResult]:
        """
        Retrieve candidates from unified collection using strategy and filters.
        
        Args:
            enhanced_query: Enhanced query string
            context: Unified retrieval context
            metadata_filters: Metadata filters for unified collection
            strategy: Unified retrieval strategy to use
            
        Returns:
            List of candidate retrieval results
        """
        if strategy == UnifiedRetrievalStrategy.AGENT_SPECIALIZED:
            return self._agent_specialized_unified_retrieval(enhanced_query, context, metadata_filters)
        elif strategy == UnifiedRetrievalStrategy.CONCEPT_FOCUSED:
            return self._concept_focused_unified_retrieval(enhanced_query, context, metadata_filters)
        elif strategy == UnifiedRetrievalStrategy.EDUCATIONAL_RELEVANCE:
            return self._educational_relevance_unified_retrieval(enhanced_query, context, metadata_filters)
        elif strategy == UnifiedRetrievalStrategy.CONTEXTUAL_ADAPTIVE:
            return self._contextual_adaptive_unified_retrieval(enhanced_query, context, metadata_filters)
        else:
            return self._metadata_filtered_unified_retrieval(enhanced_query, context, metadata_filters)
    
    def _metadata_filtered_unified_retrieval(self, 
                                           query: str, 
                                           context: UnifiedRetrievalContext,
                                           filters: Dict[str, Any]) -> List[RetrievalResult]:
        """Basic unified retrieval with metadata filtering."""
        agent_spec = None
        if context.agent_specialization:
            try:
                # DEBUG: Log what we're trying to convert
                self.logger.log_event(
                    EventType.KNOWLEDGE_RETRIEVED,
                    f"Converting agent_specialization: '{context.agent_specialization}' to enum",
                    extra_data={
                        "input_value": context.agent_specialization,
                        "input_type": type(context.agent_specialization).__name__,
                        "available_values": [e.value for e in AgentSpecialization]
                    }
                )
                agent_spec = AgentSpecialization(context.agent_specialization)
                
                # DEBUG: Log success
                self.logger.log_event(
                    EventType.KNOWLEDGE_RETRIEVED,
                    f"Successfully converted to AgentSpecialization: {agent_spec}",
                    extra_data={"agent_spec": agent_spec.value}
                )
            except Exception as e:
                # DEBUG: Log the exact error
                self.logger.log_event(
                    EventType.ERROR_OCCURRED,
                    f"Failed to convert agent_specialization: {str(e)}",
                    level="ERROR",
                    extra_data={
                        "input_value": context.agent_specialization,
                        "error_type": type(e).__name__,
                        "available_enum_values": [e.value for e in AgentSpecialization]
                    }
                )
                raise
        
        content_type = None
        if filters.get('content_type'):
            content_type = ContentType(filters['content_type'])
        
        # RESEARCH FIX: Remove all restrictive filtering for better research demonstration
        return self.vector_store.search_similar_content(
            query=query,
            agent_specialization=None,  # Don't filter by agent - let scoring handle preference
            content_type_filter=None,   # Don't filter by content type - get diverse results
            difficulty_level=None,      # Don't filter by difficulty - let scoring handle preference
            max_results=max(context.max_results * 6, 15),  # Get many more for research demo
            similarity_threshold=0.0    # Very low threshold to get broad set of candidates
        )
    
    def _agent_specialized_unified_retrieval(self, 
                                           query: str,
                                           context: UnifiedRetrievalContext,
                                           filters: Dict[str, Any]) -> List[RetrievalResult]:
        """Agent-specialized retrieval from unified collection."""
        try:
            self.logger.log_event(
                EventType.KNOWLEDGE_RETRIEVED,
                f"Converting agent_specialization: '{context.agent_specialization}' to enum",
                extra_data={
                    "input_value": context.agent_specialization,
                    "input_type": type(context.agent_specialization).__name__,
                    "available_values": [e.value for e in AgentSpecialization]
                }
            )
            agent_spec = AgentSpecialization(context.agent_specialization or "shared")
            self.logger.log_event(
                EventType.KNOWLEDGE_RETRIEVED,
                f"Successfully converted to agent_spec: {agent_spec.value}"
            )
        except Exception as e:
            self.logger.log_event(
                EventType.ERROR_OCCURRED,
                f"Failed to convert agent_specialization to enum: {str(e)}",
                level="ERROR",
                extra_data={
                    "input_value": context.agent_specialization,
                    "error_type": type(e).__name__,
                    "available_enum_values": [e.value for e in AgentSpecialization]
                }
            )
            # Fallback to shared if conversion fails
            agent_spec = AgentSpecialization.SHARED
        
        # Determine preferred content types based on agent specialization
        content_type_filter = None
        if agent_spec == AgentSpecialization.IMPLEMENTATION:
            content_type_filter = ContentType.IMPLEMENTATION_GUIDE
        elif agent_spec == AgentSpecialization.DEBUGGING:
            content_type_filter = ContentType.DEBUGGING_RESOURCE
        
        try:
            self.logger.log_event(
                EventType.KNOWLEDGE_RETRIEVED,
                f"Calling vector_store.search_similar_content with agent_spec: {agent_spec.value}"
            )
            # RESEARCH FIX: Remove restrictive filtering for better research demonstration  
            results = self.vector_store.search_similar_content(
                query=query,
                agent_specialization=None,  # Don't filter by agent - get all relevant content
                content_type_filter=None,   # Don't filter by content type - let scoring prefer
                difficulty_level=None,      # Don't filter by difficulty - let scoring prefer
                max_results=max(context.max_results * 6, 15),  # Get many more for research demo
                similarity_threshold=0.0    # Very low threshold for broad candidate set
            )
            self.logger.log_event(
                EventType.KNOWLEDGE_RETRIEVED,
                f"Vector store search returned {len(results)} results"
            )
            return results
        except Exception as e:
            import traceback
            self.logger.log_event(
                EventType.ERROR_OCCURRED,
                f"Error in vector_store.search_similar_content: {str(e)}",
                level="ERROR",
                extra_data={
                    "agent_spec_value": agent_spec.value,
                    "content_type_filter": content_type_filter.value if content_type_filter else None,
                    "difficulty_level": filters.get('difficulty_level'),
                    "traceback": traceback.format_exc()
                }
            )
            raise
    
    def _concept_focused_unified_retrieval(self, 
                                         query: str,
                                         context: UnifiedRetrievalContext,
                                         filters: Dict[str, Any]) -> List[RetrievalResult]:
        """Concept-focused retrieval from unified collection."""
        # Extract programming concepts from query
        concepts = self._extract_programming_concepts_from_query(query)
        
        agent_spec = None
        if context.agent_specialization:
            agent_spec = AgentSpecialization(context.agent_specialization or "shared")
        
        return self.vector_store.search_similar_content(
            query=query,
            agent_specialization=agent_spec,
            content_type_filter=ContentType.CONCEPT_EXPLANATION,
            programming_concepts=concepts[:3] if concepts else None,  # Top 3 concepts
            difficulty_level=filters.get('difficulty_level'),
            max_results=context.max_results * 3,
            similarity_threshold=context.similarity_threshold
        )
    
    def _educational_relevance_unified_retrieval(self, 
                                               query: str,
                                               context: UnifiedRetrievalContext,
                                               filters: Dict[str, Any]) -> List[RetrievalResult]:
        """Educational relevance focused retrieval from unified collection."""
        # Multiple searches with different educational content types
        all_results = []
        
        agent_spec = None
        if context.agent_specialization:
            agent_spec = AgentSpecialization(context.agent_specialization or "shared")
        
        # Search for different educational content types
        educational_types = [
            ContentType.CONCEPT_EXPLANATION,
            ContentType.CODE_EXAMPLE,
            ContentType.BEST_PRACTICE,
            ContentType.EXERCISE
        ]
        
        for content_type in educational_types:
            results = self.vector_store.search_similar_content(
                query=query,
                agent_specialization=agent_spec,
                content_type_filter=content_type,
                difficulty_level=filters.get('difficulty_level'),
                max_results=context.max_results,
                similarity_threshold=context.similarity_threshold * 0.8  # Lower threshold
            )
            all_results.extend(results)
        
        # Remove duplicates and return top results
        unique_results = self._deduplicate_results(all_results)
        return unique_results[:context.max_results * 4]
    
    def _contextual_adaptive_unified_retrieval(self, 
                                             query: str,
                                             context: UnifiedRetrievalContext,
                                             filters: Dict[str, Any]) -> List[RetrievalResult]:
        """Advanced contextual adaptive retrieval from unified collection."""
        all_results = []
        
        # Multiple retrieval passes with different strategies
        strategies = [
            ("main_query", query),
            ("code_focused", self._create_code_focused_query(query, context)),
            ("concept_focused", self._create_concept_focused_query(query, context)),
            ("error_focused", self._create_error_focused_query(query, context))
        ]
        
        agent_spec = None
        if context.agent_specialization:
            agent_spec = AgentSpecialization(context.agent_specialization or "shared")
        
        for strategy_name, search_query in strategies:
            if search_query and search_query != query:  # Avoid duplicate searches
                results = self.vector_store.search_similar_content(
                    query=search_query,
                    agent_specialization=agent_spec,
                    difficulty_level=filters.get('difficulty_level'),
                    max_results=context.max_results * 2,
                    similarity_threshold=context.similarity_threshold * 0.7  # Lower threshold
                )
                
                # Tag results with strategy for scoring
                for result in results:
                    if 'retrieval_strategy' not in result.metadata:
                        result.metadata['retrieval_strategy'] = strategy_name
                
                all_results.extend(results)
        
        # Deduplicate and return
        unique_results = self._deduplicate_results(all_results)
        return unique_results[:context.max_results * 5]
    
    def _score_unified_educational_relevance(self, 
                                           candidates: List[RetrievalResult],
                                           context: UnifiedRetrievalContext) -> List[UnifiedScoredResult]:
        """
        Score candidates based on unified educational relevance metrics.
        
        Args:
            candidates: Candidate retrieval results from unified collection
            context: Unified retrieval context
            
        Returns:
            List of scored results with comprehensive unified scoring
        """
        scored_results = []
        
        for candidate in candidates:
            # Initialize scored result
            scored_result = UnifiedScoredResult(result=candidate)
            
            # Base similarity score
            scored_result.similarity_score = candidate.similarity_score
            
            # Metadata match scoring (new for unified approach)
            scored_result.metadata_match_score = self._calculate_metadata_match(candidate, context)
            
            # Educational relevance scoring
            scored_result.educational_relevance_score = self._calculate_educational_relevance(
                candidate, context
            )
            
            # Context match scoring
            scored_result.context_match_score = self._calculate_context_match(candidate, context)
            
            # Agent specialization match scoring
            scored_result.agent_specialization_score = self._calculate_agent_specialization_match(
                candidate, context
            )
            
            # Quality scoring
            scored_result.quality_score = self._calculate_quality_score(candidate)
            
            # Freshness scoring (based on last accessed, retrieval count)
            scored_result.freshness_score = self._calculate_freshness_score(candidate)
            
            # Calculate unified combined score
            scored_result.combined_score = self._calculate_unified_combined_score(scored_result)
            
            # Store comprehensive score explanation
            scored_result.score_explanation = {
                "similarity": scored_result.similarity_score,
                "metadata_match": scored_result.metadata_match_score,
                "educational_relevance": scored_result.educational_relevance_score,
                "context_match": scored_result.context_match_score,
                "agent_specialization": scored_result.agent_specialization_score,
                "quality": scored_result.quality_score,
                "freshness": scored_result.freshness_score,
                "combined": scored_result.combined_score
            }
            
            scored_results.append(scored_result)
        
        return scored_results
    
    def _calculate_metadata_match(self, candidate: RetrievalResult, context: UnifiedRetrievalContext) -> float:
        """Calculate how well candidate metadata matches context requirements."""
        match_score = 0.0
        
        # Agent specialization match
        if context.agent_specialization and candidate.agent_specialization == context.agent_specialization:
            match_score += 0.3
        
        # Difficulty level match
        if context.student_level and candidate.difficulty_level == context.student_level:
            match_score += 0.2
        
        # Content type preference match
        if context.content_type_preference and candidate.content_type == context.content_type_preference:
            match_score += 0.2
        
        # Code examples preference match
        if context.prefer_code_examples and candidate.metadata.get('has_code_examples', False):
            match_score += 0.15
        
        # Error examples preference match
        if context.prefer_error_examples and candidate.metadata.get('has_error_examples', False):
            match_score += 0.15
        
        return min(1.0, match_score)
    
    def _calculate_educational_relevance(self, candidate: RetrievalResult, context: UnifiedRetrievalContext) -> float:
        """Calculate educational relevance score for unified approach."""
        relevance_score = 0.0
        
        # Programming concepts overlap
        if context.programming_domain:
            if context.programming_domain.lower() in [concept.lower() for concept in candidate.programming_concepts]:
                relevance_score += 0.25
        
        # Learning objectives alignment
        if context.learning_objectives:
            candidate_content_lower = candidate.content.lower()
            objectives_found = sum(1 for obj in context.learning_objectives 
                                 if obj.lower() in candidate_content_lower)
            relevance_score += min(0.3, objectives_found * 0.1)
        
        # SRL phase alignment
        if context.srl_phase:
            if context.srl_phase == SRLPhase.FORETHOUGHT.value:
                forethought_indicators = ["plan", "design", "approach", "strategy", "implement"]
                if any(indicator in candidate.content.lower() for indicator in forethought_indicators):
                    relevance_score += 0.2
            elif context.srl_phase == SRLPhase.PERFORMANCE.value:
                performance_indicators = ["debug", "error", "fix", "troubleshoot", "problem"]
                if any(indicator in candidate.content.lower() for indicator in performance_indicators):
                    relevance_score += 0.2
        
        # Content quality indicators
        quality_indicators = ["example", "step", "guide", "tutorial", "explanation"]
        quality_found = sum(1 for indicator in quality_indicators 
                          if indicator in candidate.content.lower())
        relevance_score += min(0.25, quality_found * 0.05)
        
        return min(1.0, relevance_score)
    
    def _calculate_context_match(self, candidate: RetrievalResult, context: UnifiedRetrievalContext) -> float:
        """Calculate context match score."""
        context_score = 0.0
        
        # Code snippet context match
        if context.code_snippet:
            code_terms = self._extract_code_terms(context.code_snippet)
            candidate_content_lower = candidate.content.lower()
            terms_found = sum(1 for term in code_terms if term.lower() in candidate_content_lower)
            context_score += min(0.4, terms_found * 0.1)
        
        # Error message context match
        if context.error_message:
            error_type = self._extract_error_type(context.error_message)
            if error_type and error_type.lower() in candidate.content.lower():
                context_score += 0.3
        
        # Conversation history context
        if context.conversation_history:
            recent_topics = self._extract_topics_from_history(context.conversation_history)
            candidate_content_lower = candidate.content.lower()
            topics_found = sum(1 for topic in recent_topics if topic.lower() in candidate_content_lower)
            context_score += min(0.3, topics_found * 0.1)
        
        return min(1.0, context_score)
    
    def _calculate_agent_specialization_match(self, candidate: RetrievalResult, context: UnifiedRetrievalContext) -> float:
        """Calculate agent specialization match score."""
        if not context.agent_specialization:
            return 0.5  # Neutral score if no preference
        
        if candidate.agent_specialization == context.agent_specialization:
            return 1.0
        elif candidate.agent_specialization == AgentSpecialization.SHARED.value:
            return 0.7  # Shared content is good for any agent
        else:
            return 0.3  # Different specialization, but still potentially useful
    
    def _calculate_quality_score(self, candidate: RetrievalResult) -> float:
        """Calculate content quality score."""
        quality_score = 0.5  # Base score
        
        # Content length quality (not too short, not too long)
        content_length = candidate.metadata.get('content_length', 0)
        if 200 <= content_length <= 2000:
            quality_score += 0.2
        elif content_length > 100:
            quality_score += 0.1
        
        # Code examples boost quality
        if candidate.metadata.get('has_code_examples', False):
            quality_score += 0.15
        
        # Educational value indicators
        educational_words = ["example", "explanation", "guide", "tutorial", "step"]
        content_lower = candidate.content.lower()
        educational_indicators = sum(1 for word in educational_words if word in content_lower)
        quality_score += min(0.15, educational_indicators * 0.03)
        
        return min(1.0, quality_score)
    
    def _calculate_freshness_score(self, candidate: RetrievalResult) -> float:
        """Calculate content freshness score."""
        # Higher score for recently accessed, frequently retrieved content
        retrieval_count = candidate.metadata.get('retrieval_count', 0)
        last_accessed = candidate.metadata.get('last_accessed', 0)
        
        # Retrieval frequency component
        frequency_score = min(0.5, retrieval_count * 0.1)
        
        # Recency component (simple linear decay)
        current_time = time.time()
        time_since_access = current_time - last_accessed
        days_since_access = time_since_access / (24 * 3600)
        recency_score = max(0.0, 0.5 - (days_since_access * 0.05))
        
        return frequency_score + recency_score
    
    def _calculate_unified_combined_score(self, scored_result: UnifiedScoredResult) -> float:
        """Calculate unified combined score using weighted components."""
        weights = self.educational_weights
        
        combined_score = (
            scored_result.similarity_score * weights.get('similarity', 0.25) +
            scored_result.metadata_match_score * weights.get('metadata_match', 0.20) +
            scored_result.educational_relevance_score * weights.get('educational_relevance', 0.20) +
            scored_result.context_match_score * weights.get('context_match', 0.15) +
            scored_result.agent_specialization_score * weights.get('agent_specialization', 0.10) +
            scored_result.quality_score * weights.get('quality', 0.07) +
            scored_result.freshness_score * weights.get('freshness', 0.03)
        )
        
        return min(1.0, combined_score)
    
    def _apply_unified_contextual_filtering(self, 
                                          scored_results: List[UnifiedScoredResult],
                                          context: UnifiedRetrievalContext) -> List[UnifiedScoredResult]:
        """Apply advanced contextual filtering for unified results."""
        # DEBUGGING: Log input for contextual filtering
        self.logger.log_event(
            EventType.KNOWLEDGE_RETRIEVED,
            "CONTEXTUAL FILTERING - BEFORE",
            extra_data={
                "input_results_count": len(scored_results),
                "similarity_threshold": context.similarity_threshold,
                "query": context.query[:100]
            }
        )
        
        # RESEARCH DEMO FIX: Make filtering much more permissive to demonstrate RAG functionality
        filtered_results = []
        similarity_failures = 0
        quality_failures = 0 
        relevance_failures = 0
        
        for i, scored_result in enumerate(scored_results):
            result_id = scored_result.result.content_id
            
            # Very permissive similarity threshold for demo
            similarity_threshold = max(0.01, context.similarity_threshold)
            if scored_result.similarity_score < similarity_threshold:
                similarity_failures += 1
                self.logger.log_event(
                    EventType.KNOWLEDGE_RETRIEVED,
                    f"FILTERED OUT - Similarity too low: {result_id}",
                    extra_data={
                        "similarity_score": scored_result.similarity_score,
                        "threshold": similarity_threshold,
                        "result_position": i
                    }
                )
                continue
            
            # Much more permissive quality threshold for demo
            quality_threshold = 0.1  # Reduced from 0.3
            if scored_result.quality_score < quality_threshold:
                quality_failures += 1
                self.logger.log_event(
                    EventType.KNOWLEDGE_RETRIEVED,
                    f"FILTERED OUT - Quality too low: {result_id}",
                    extra_data={
                        "quality_score": scored_result.quality_score,
                        "threshold": quality_threshold,
                        "result_position": i
                    }
                )
                continue
            
            # Much more permissive educational relevance for demo  
            relevance_threshold = 0.05  # Reduced from 0.2
            if scored_result.educational_relevance_score < relevance_threshold:
                relevance_failures += 1
                self.logger.log_event(
                    EventType.KNOWLEDGE_RETRIEVED,
                    f"FILTERED OUT - Educational relevance too low: {result_id}",
                    extra_data={
                        "educational_relevance_score": scored_result.educational_relevance_score,
                        "threshold": relevance_threshold,
                        "result_position": i
                    }
                )
                continue
            
            # Always pass context filters for demo purposes
            self.logger.log_event(
                EventType.KNOWLEDGE_RETRIEVED,
                f"PASSED ALL FILTERS: {result_id}",
                extra_data={
                    "similarity_score": scored_result.similarity_score,
                    "quality_score": scored_result.quality_score,
                    "educational_relevance_score": scored_result.educational_relevance_score,
                    "combined_score": scored_result.combined_score,
                    "result_position": i
                }
            )
            filtered_results.append(scored_result)
        
        # If still no results, return top results anyway for demo
        if not filtered_results and scored_results:
            self.logger.log_event(
                EventType.KNOWLEDGE_RETRIEVED,
                "NO RESULTS PASSED FILTERS - Falling back to top similarity results",
                extra_data={
                    "original_count": len(scored_results),
                    "fallback_count": min(3, len(scored_results))
                }
            )
            # Sort by similarity and take top results
            sorted_results = sorted(scored_results, key=lambda x: x.similarity_score, reverse=True)
            filtered_results = sorted_results[:min(3, len(sorted_results))]
        
        # DEBUGGING: Log final filtering results
        self.logger.log_event(
            EventType.KNOWLEDGE_RETRIEVED,
            "RESEARCH CONTEXTUAL FILTERING - RESULTS",
            extra_data={
                "input_count": len(scored_results),
                "tier_1_count": len([r for r in scored_results if r.similarity_score >= 0.3 and r.quality_score >= 0.4]),
                "tier_2_count": len([r for r in scored_results if r.similarity_score >= 0.1 and r.quality_score >= 0.2]),
                "tier_3_count": len([r for r in scored_results if r.similarity_score >= 0.05 and r.quality_score >= 0.1]),
                "final_output_count": len(filtered_results),
                "approach": "research_permissive_filtering"
            }
        )
        
        return filtered_results
    
    def _passes_contextual_filters(self, scored_result: UnifiedScoredResult, context: UnifiedRetrievalContext) -> bool:
        """Check if result passes context-specific filters."""
        # Agent specialization filter
        if context.agent_specialization:
            if (scored_result.result.agent_specialization != context.agent_specialization and 
                scored_result.result.agent_specialization != AgentSpecialization.SHARED.value):
                return scored_result.agent_specialization_score > 0.5
        
        # Content type preference filter
        if context.content_type_preference:
            if scored_result.result.content_type != context.content_type_preference:
                return scored_result.metadata_match_score > 0.6
        
        # Code examples requirement
        if context.prefer_code_examples:
            if not scored_result.result.metadata.get('has_code_examples', False):
                return scored_result.educational_relevance_score > 0.7
        
        return True
    
    def _unified_final_ranking_and_selection(self, 
                                           filtered_results: List[UnifiedScoredResult],
                                           context: UnifiedRetrievalContext) -> List[UnifiedScoredResult]:
        """Final ranking and selection for unified results."""
        # Sort by combined score
        sorted_results = sorted(filtered_results, key=lambda x: x.combined_score, reverse=True)
        
        # Assign rank positions
        for i, result in enumerate(sorted_results):
            result.rank_position = i + 1
        
        # Apply diversity filtering to avoid too similar results
        diverse_results = self._apply_diversity_filtering(sorted_results, context)
        
        # Select top results
        final_results = diverse_results[:context.max_results]
        
        return final_results
    
    def _apply_diversity_filtering(self, 
                                 sorted_results: List[UnifiedScoredResult],
                                 context: UnifiedRetrievalContext) -> List[UnifiedScoredResult]:
        """Apply diversity filtering to avoid overly similar results."""
        if len(sorted_results) <= context.max_results:
            return sorted_results
        
        diverse_results = []
        content_similarity_threshold = 0.85
        
        for result in sorted_results:
            is_diverse = True
            
            # Check similarity with already selected results
            for selected in diverse_results:
                content_similarity = self._calculate_content_similarity(
                    result.result.content, selected.result.content
                )
                
                if content_similarity > content_similarity_threshold:
                    is_diverse = False
                    break
            
            if is_diverse:
                diverse_results.append(result)
            
            # Stop if we have enough diverse results
            if len(diverse_results) >= context.max_results * 2:
                break
        
        return diverse_results
    
    def _calculate_content_similarity(self, content1: str, content2: str) -> float:
        """Calculate similarity between two content strings."""
        # Simple word overlap similarity
        words1 = set(content1.lower().split())
        words2 = set(content2.lower().split())
        
        if not words1 or not words2:
            return 0.0
        
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        return len(intersection) / len(union) if union else 0.0
    
    def _update_unified_retrieval_metrics(self, 
                                        strategy: UnifiedRetrievalStrategy,
                                        results: List[UnifiedScoredResult],
                                        retrieval_time: float,
                                        metadata_filters: Dict[str, Any]):
        """Update unified retrieval performance metrics."""
        # Update basic metrics
        self.retrieval_metrics.total_retrievals += 1
        self.retrieval_metrics.average_retrieval_time_ms = (
            (self.retrieval_metrics.average_retrieval_time_ms * (self.retrieval_metrics.total_retrievals - 1) + 
             retrieval_time) / self.retrieval_metrics.total_retrievals
        )
        self.retrieval_metrics.average_results_returned = (
            (self.retrieval_metrics.average_results_returned * (self.retrieval_metrics.total_retrievals - 1) + 
             len(results)) / self.retrieval_metrics.total_retrievals
        )
        
        # Update quality metrics
        if results:
            avg_similarity = sum(r.similarity_score for r in results) / len(results)
            avg_metadata_match = sum(r.metadata_match_score for r in results) / len(results)
            avg_educational_relevance = sum(r.educational_relevance_score for r in results) / len(results)
            
            self.retrieval_metrics.average_similarity_score = (
                (self.retrieval_metrics.average_similarity_score * (self.retrieval_metrics.total_retrievals - 1) + 
                 avg_similarity) / self.retrieval_metrics.total_retrievals
            )
            self.retrieval_metrics.average_metadata_match_score = (
                (self.retrieval_metrics.average_metadata_match_score * (self.retrieval_metrics.total_retrievals - 1) + 
                 avg_metadata_match) / self.retrieval_metrics.total_retrievals
            )
            self.retrieval_metrics.average_educational_relevance = (
                (self.retrieval_metrics.average_educational_relevance * (self.retrieval_metrics.total_retrievals - 1) + 
                 avg_educational_relevance) / self.retrieval_metrics.total_retrievals
            )
        
        # Update strategy usage
        strategy_name = strategy.value
        self.retrieval_metrics.strategy_usage[strategy_name] = (
            self.retrieval_metrics.strategy_usage.get(strategy_name, 0) + 1
        )
        
        # Update filter usage
        for filter_name in metadata_filters.keys():
            self.retrieval_metrics.filter_usage[filter_name] = (
                self.retrieval_metrics.filter_usage.get(filter_name, 0) + 1
            )
    
    def _extract_code_terms(self, code_snippet: str) -> List[str]:
        """Extract key terms from code snippet."""
        # Extract function names, variable names, keywords
        terms = []
        
        # Function definitions
        function_matches = re.findall(r'def\s+(\w+)', code_snippet)
        terms.extend(function_matches)
        
        # Class definitions
        class_matches = re.findall(r'class\s+(\w+)', code_snippet)
        terms.extend(class_matches)
        
        # Import statements
        import_matches = re.findall(r'import\s+(\w+)', code_snippet)
        terms.extend(import_matches)
        
        # Variable assignments
        var_matches = re.findall(r'(\w+)\s*=', code_snippet)
        terms.extend(var_matches)
        
        return list(set(terms))  # Remove duplicates
    
    def _extract_error_type(self, error_message: str) -> Optional[str]:
        """Extract error type from error message."""
        error_patterns = [
            r'(\w*Error)', r'(\w*Exception)', r'(\w*Warning)'
        ]
        
        for pattern in error_patterns:
            match = re.search(pattern, error_message)
            if match:
                return match.group(1)
        
        return None
    
    def _extract_topics_from_history(self, conversation_history: List[Dict[str, str]]) -> List[str]:
        """Extract key topics from conversation history."""
        topics = []
        
        for message in conversation_history[-3:]:  # Last 3 messages
            content = message.get('content', '')
            # Extract key programming terms
            programming_terms = re.findall(r'\b(function|class|variable|array|list|loop|condition)\b', content.lower())
            topics.extend(programming_terms)
        
        return list(set(topics))
    
    def _extract_programming_concepts_from_query(self, query: str) -> List[str]:
        """Extract programming concepts from query."""
        concepts = []
        query_lower = query.lower()
        
        # Check against concept patterns
        concept_keywords = {
            "algorithms": ["algorithm", "sort", "search", "recursive", "iterate"],
            "data_structures": ["array", "list", "stack", "queue", "tree", "graph"],
            "object_oriented": ["class", "object", "inherit", "polymorph", "encapsulat"],
            "control_flow": ["loop", "condition", "if", "while", "for"],
            "functions": ["function", "method", "parameter", "return"],
            "error_handling": ["error", "exception", "debug", "troubleshoot"]
        }
        
        for concept, keywords in concept_keywords.items():
            if any(keyword in query_lower for keyword in keywords):
                concepts.append(concept)
        
        return concepts
    
    def _create_code_focused_query(self, query: str, context: UnifiedRetrievalContext) -> str:
        """Create code-focused query variant."""
        if not context.code_snippet:
            return query
        
        code_terms = self._extract_code_terms(context.code_snippet)
        if code_terms:
            return f"{query} {' '.join(code_terms[:3])} code example implementation"
        
        return query
    
    def _create_concept_focused_query(self, query: str, context: UnifiedRetrievalContext) -> str:
        """Create concept-focused query variant."""
        concepts = self._extract_programming_concepts_from_query(query)
        if concepts:
            return f"{query} {' '.join(concepts)} concept explanation theory"
        
        return f"{query} concept explanation understand"
    
    def _create_error_focused_query(self, query: str, context: UnifiedRetrievalContext) -> str:
        """Create error-focused query variant."""
        if not context.error_message:
            return query
        
        error_type = self._extract_error_type(context.error_message)
        if error_type:
            return f"{query} {error_type} error debug fix troubleshoot"
        
        return f"{query} error debug troubleshoot"
    
    def _deduplicate_results(self, results: List[RetrievalResult]) -> List[RetrievalResult]:
        """Remove duplicate results based on content ID."""
        seen_ids = set()
        unique_results = []
        
        for result in results:
            if result.content_id not in seen_ids:
                seen_ids.add(result.content_id)
                unique_results.append(result)
        
        return unique_results
    
    def _expand_keywords(self, query: str) -> str:
        """Expand keywords for better retrieval."""
        # Simple keyword expansion
        expansions = {
            "implement": "implement implementation build create develop",
            "debug": "debug debugging troubleshoot fix error",
            "algorithm": "algorithm method approach technique",
            "function": "function method procedure routine",
            "error": "error exception bug issue problem"
        }
        
        query_words = query.lower().split()
        expanded_words = []
        
        for word in query_words:
            expanded_words.append(word)
            if word in expansions:
                expanded_words.append(expansions[word])
        
        return " ".join(expanded_words)
    
    def _initialize_strategy_weights(self) -> Dict[str, float]:
        """Initialize unified strategy weights."""
        return {
            "metadata_filtered": 0.7,
            "agent_specialized": 0.9,
            "concept_focused": 0.85,
            "educational_relevance": 0.8,
            "contextual_adaptive": 0.95
        }
    
    def _initialize_educational_weights(self) -> Dict[str, float]:
        """Initialize educational scoring weights for unified approach."""
        return {
            "similarity": 0.25,
            "metadata_match": 0.20,
            "educational_relevance": 0.20,
            "context_match": 0.15,
            "agent_specialization": 0.10,
            "quality": 0.07,
            "freshness": 0.03
        }
    
    def _initialize_metadata_weights(self) -> Dict[str, float]:
        """Initialize metadata filtering weights."""
        return {
            "agent_specialization": 0.3,
            "content_type": 0.25,
            "difficulty_level": 0.2,
            "programming_concepts": 0.15,
            "has_code_examples": 0.05,
            "has_error_examples": 0.05
        }
    
    def _initialize_concept_similarity(self) -> Dict[str, List[str]]:
        """Initialize concept similarity mappings."""
        return {
            "algorithms": ["sorting", "searching", "recursion", "iteration"],
            "data_structures": ["arrays", "lists", "trees", "graphs", "stacks", "queues"],
            "programming": ["coding", "development", "implementation", "software"],
            "debugging": ["troubleshooting", "error_handling", "testing", "fixing"]
        }
    
    def _initialize_keyword_expansions(self) -> Dict[str, List[str]]:
        """Initialize keyword expansion mappings."""
        return {
            "implement": ["build", "create", "develop", "code", "write"],
            "debug": ["troubleshoot", "fix", "resolve", "diagnose"],
            "understand": ["learn", "comprehend", "grasp", "know"],
            "example": ["sample", "demonstration", "illustration", "instance"]
        }
    
    def get_retrieval_metrics(self) -> UnifiedRetrievalMetrics:
        """Get unified retrieval performance metrics."""
        return self.retrieval_metrics


# Global unified educational retriever instance
_unified_educational_retriever: Optional[UnifiedEducationalRetriever] = None


def get_educational_retriever(reload: bool = False) -> UnifiedEducationalRetriever:
    """
    Get global unified educational retriever instance (singleton pattern).
    
    Args:
        reload: Force creation of new retriever instance
        
    Returns:
        UnifiedEducationalRetriever instance
    """
    global _unified_educational_retriever
    if _unified_educational_retriever is None or reload:
        _unified_educational_retriever = UnifiedEducationalRetriever()
    return _unified_educational_retriever


if __name__ == "__main__":
    # Unified educational retriever test
    try:
        retriever = get_educational_retriever()
        
        # Test unified retrieval context
        test_context = UnifiedRetrievalContext(
            query="How do I implement binary search algorithm?",
            agent_specialization=AgentSpecialization.IMPLEMENTATION.value,
            srl_phase=SRLPhase.FORETHOUGHT.value,
            student_level="intermediate",
            prefer_code_examples=True,
            programming_domain="algorithms",
            learning_objectives=["understand binary search", "implement efficiently"],
            max_results=5
        )
        
        # Perform unified retrieval
        results = retriever.retrieve_educational_content(test_context)
        
        print(f"Unified retrieval results: {len(results)} found")
        for i, result in enumerate(results):
            print(f"  {i+1}. {result.result.content_id}")
            print(f"     Similarity: {result.similarity_score:.3f}")
            print(f"     Metadata Match: {result.metadata_match_score:.3f}")
            print(f"     Educational Relevance: {result.educational_relevance_score:.3f}")
            print(f"     Combined Score: {result.combined_score:.3f}")
            print(f"     Content Type: {result.result.content_type}")
            print(f"     Agent Specialization: {result.result.agent_specialization}")
            print()
        
        # Test metrics
        metrics = retriever.get_retrieval_metrics()
        print(f"Unified retrieval metrics:")
        print(f"  - Total retrievals: {metrics.total_retrievals}")
        print(f"  - Average time: {metrics.average_retrieval_time_ms:.2f}ms")
        print(f"  - Average results: {metrics.average_results_returned:.1f}")
        print(f"  - Strategy usage: {metrics.strategy_usage}")
        
        print(" Unified educational retriever test completed successfully!")
        
    except Exception as e:
        print(f" Unified educational retriever test failed: {e}")
        import traceback
        traceback.print_exc()
